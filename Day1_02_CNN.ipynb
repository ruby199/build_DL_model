{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### CNN for Feature Extraction \n",
        "* image/video - \n",
        "e.g. pinterest(hashtag as labeling-supervise learning)\n",
        "e.g. Adobe DeepFont (to detect specific designed font)\n",
        "\n",
        "* paper review youtube channel:\n",
        "https://www.youtube.com/@user-ow3gm9zd1b/featured\n",
        "\n",
        "\n",
        "* CNN (mostly used in image classification)\n",
        "* Fully-connected layer (어디 위치에 쓰이느냐에 따라 이름이 달라진다)"
      ],
      "metadata": {
        "id": "wlo6XWvRSxtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Terminology\n",
        "- **channel**: A channel is like a separate color or information in an image. In color images, there are different channels for red, green, and blue. Each channel holds specific color information.\n",
        "\n",
        "- **Filter**: A filter is like a special tool used to modify an image. It can do things like making an image blurry, enhancing edges, or finding specific patterns. The filter uses a set of numbers to determine how it changes the image. 이미지에서 필요한/주요한 특징값만 추출하고 필요없는 특징값은 버리는 역할. CNN에서 feature값을 보고 판단/예측할 수 있다. filter안의 값들 = weight 값. *filter의 개수는 많을수록 좋고, 깊이도 깊을수록 더 좋다.* 중심부에서 더 많은 feature를 받게 된다.\n",
        "\n",
        "- Stride: How far to go to the right/bottom to perform the next convolution (중앙값 뿐만 아니라 다른 곳에도 가중치를 두기 위해서, 즉 몇칸씩 이동할지)\n",
        "\n",
        "- **Feature map**: A feature map is like a map that highlights specific features in an image. It's created by applying filters to the image. Each feature map shows the presence or intensity of a particular feature, such as edges or textures.\n",
        "\n",
        "- **padding**: Padding is like adding extra space around an image. It helps to keep the image size consistent during certain operations. Padding can be done by adding extra pixels with specific values, like zeros, around the edges of the image.\n",
        "\n",
        "- zero padding (0으로 테두리):  It involves adding extra rows and columns of zeros around the edges of an image or feature map before applying convolutional operations. 중앙필터 방지, & 가중으로 필터를 씌워야할 필요가 있을 때 - 이미지의 작아짐 방지. "
      ],
      "metadata": {
        "id": "g_Oj1vxHS2iT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN composed layers\n",
        "- Convolutional Layer\n",
        "- Fully Connected Layer\n",
        "- Pooling Layer (cleaning up 정리정돈) - Max Pooling, Average Pooling. 짧은 시간안에 목표까지 도달할 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "EQciuyvxS7KL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification\n",
        "CNN: 영상 이미지 뿐만 아니라 모든 matrix data 가능, including text data\n",
        "test -> 의미를 가지고 있는 vector값으로 표현...\n",
        "\n",
        "\n",
        "### <추세>  \n",
        "~2019 회귀 MLP. 이미지 CNN -> MLP MIXER , 자연어/시계열 RNN  -> CNN-> CNN + RNN\n",
        "\n",
        "2021~\n",
        "image, video - transformer (자연어에서 사용하던 transformer가 영상쪽에서 사용되자 기존 CNN 모두 이겨버림! 최고 성능을 내는 것은 결국 transformer이다)\n",
        "BUT transformer (CNN보다 훨씬 더 많은 cost) - 1%~2% +성능이기 때문에 실무에서는 더 작은 model인 CNN 선호.\n",
        "\n",
        "-> CNN + transformer (기반 model - 엄청난 성능, 그러나 엄청나게 크다) - 더 큰 transformer 모델이 무조건 성능이 좋기 때문에 성능적 개선을 연구하는 움직임은 침체. - 이제는 data적인 부분 (labeled/unlabeled data) 등 실무적인 부분에 대한 연구로 전향되고 있다. "
      ],
      "metadata": {
        "id": "ECJoqpRZS-wU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST dataset\n",
        "- CNN사용 유무에 따른 연산량의 차이, 속도 차이 비교/연구\n",
        "\n",
        "- Difference in computational complexity and speed comparison/research depending on the use of CNN.\n",
        "\n",
        "- MNIST data: 다양한 case, data class의 정확성, data가 많고 다양하다. 실무에서 좋은 data 확보하는 것이 매우 중요하다!"
      ],
      "metadata": {
        "id": "Oxj301MoSjBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI-OcQNcSj8j",
        "outputId": "379cf6fb-75a9-4de2-bfd4-1c9d2a2704fd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist"
      ],
      "metadata": {
        "id": "rE-RLKVzSnAj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "xkWnN71cTNdE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "# X_train, X_test norm(0-1)"
      ],
      "metadata": {
        "id": "LjtKt9F4VMMS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### try training without CNN"
      ],
      "metadata": {
        "id": "mcRsr8V4WR_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "#  multidimensional feature maps are reshaped into a single vector,(Here, it converts the 2D image into a 1D vector of size 784(28*28))\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28) )) # input shape: data하나의 크기가 관건이다. \n",
        "model.add(tf.keras.layers.Dense(128, activation='relu')) \n",
        "# input is 1D vector of size 784, each units in Dense layer has a weight for each input feature, 784*128=100,352 weights, 128 bias terms,\n",
        "# Thus, 100,352+128=100,480 parameters\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(0.2)) # 특정 feature들을 가리고 학습하는 것. to prevent overfitting. \n",
        "\n",
        "model.add(tf.keras.layers.Dense(10 , activation ='softmax')) # This layer is defined with 10 units and uses the softmax activation function.\n",
        "# input is 1D vector of size 128. Thus, 10*128 = 1,280 weights. & bias term of 10, \n",
        "# Thus, 1,280 + 10 = 1,290 parameters\n",
        "\n",
        "model.summary() \n",
        "# Total: 100,480 + 1,290 = 101,770 parameters in the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_UL-7jdWMGS",
        "outputId": "00c36c3b-eee6-4e14-ad09-ad6d0c5472a9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building loss function and optimizer\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Y2yw3NE2bs5m"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs = 5, \n",
        "          validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA121vTTWyBz",
        "outputId": "fe36edda-85cd-4d04-b098-2da2fc18e0e0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 3ms/step - loss: 0.2970 - accuracy: 0.9146 - val_loss: 0.1405 - val_accuracy: 0.9592\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1453 - accuracy: 0.9565 - val_loss: 0.0990 - val_accuracy: 0.9695\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1088 - accuracy: 0.9671 - val_loss: 0.0857 - val_accuracy: 0.9740\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0897 - accuracy: 0.9723 - val_loss: 0.0814 - val_accuracy: 0.9732\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0781 - accuracy: 0.9754 - val_loss: 0.0765 - val_accuracy: 0.9765\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f68e8b5f760>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}